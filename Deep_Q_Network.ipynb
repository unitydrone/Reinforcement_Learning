{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN을 이용해 상태마다 각 행동의 큐 함수들을 근사해줌.\n",
    "- Q Learning Algorithm : 모든 상태와 행동에 대한 Q함수 값을 따로 저장, 학습수행, 행동 결정.\n",
    "    - 단, 매우 많은 상황이 존재하는 환경에서는 적합하지 않음. \n",
    "    - ex) pong game\n",
    "    \n",
    "- 타겟(y) 네트워크, 일반 네트워크 2종류의 네트워크가 사용.\n",
    "\n",
    "    $$ y = r + \\gamma(\\max{Q}(s_{t+1}, a^\\prime;\\theta^{-})$$\n",
    "    \n",
    "$$ \\theta : \\text{일반 네트워크 변수}, \\theta^{-} : \\text{타켓 네트워크 변수} $$\n",
    "\n",
    "- 다음 Step에서 게임이 종료된 경우 게임이 계속 진행되는 경우.\n",
    "- 타겟값 y와 예측값 Q의 차이가 최소가 되도록 학습 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# DQN의 리플레이 메모리 역할 수행.\n",
    "from collections import deque\n",
    "\n",
    "# Unity로 만든 환경을 로드하기 위해 불러옴.\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN 파라미터 값 설정.\n",
    "state_size  = [84, 84, 3]\n",
    "action_size = 5\n",
    "\n",
    "load_model  = False\n",
    "\n",
    "# 에이전트 학습 수행.\n",
    "train_mode  = True\n",
    "\n",
    "batch_size      = 32\n",
    "mem_maxlen      = 50000\n",
    "\n",
    "# 에이전트가 학습을 수행할 때 얼마나 미래의 보상을 고려할지를 결정하는 감가율\n",
    "# 0 ~ 1 사이의 값으로 설정. ==> 값이 클수록 미래의 보상을 많이 고려.\n",
    "discount_factor = 0.9\n",
    "# 네트워크 학습을 얼마 빠르게 수행할지 결정하는 파라미터.\n",
    "# 값이 너무 작으면 학습속도가 느려지고, 이 값이 너무 크면 학습이 불안정하게 수행됨.\n",
    "learning_rate   = 0.00025\n",
    "\n",
    "# 총 몇 번의 에피소드 동안 학습을 수행할지를 결정.\n",
    "# 한 에피소드는 승리 혹은 패배 등의 이유로 게임이 한 번 끝나는 상황을 의미.\n",
    "run_episode  = 25000\n",
    "\n",
    "# 학습이 끝나고 테스트할 때 학습된 에이전트로 몇 에피소드 동안 게임을 진행할지를 결정.\n",
    "# 테스트의 경우 일반적으로 랜덤 행동 수행이 아닌 네트워크의 학습 결과만을 가지고 에이전트가 행동을 택함.\n",
    "test_episode = 1000\n",
    "\n",
    "# 학습을 시작하기 전에 리플레이 메모리에 충분한 데이터를 모으기 위해 몇 에피소드 동안 임의의 행동으로 게임을 진행할 것인지 결정.\n",
    "start_train_episode = 1000\n",
    "\n",
    "# 타겟 네트워크를 몇 스텝 주기로 업데이트할지를 결정.\n",
    "target_update_step = 10000\n",
    "\n",
    "# 에피소드가 진행될 때마다 학습 상황을 파악할 수 있는 지표를 출력.\n",
    "print_interval = 100\n",
    "\n",
    "# 설정된 에피소드마다 학습이 진행되고 있는 모델을 저장해 나중에 불러옴.\n",
    "# 즉, 5000 에피소드마다 저장.\n",
    "save_interval = 5000\n",
    "\n",
    "# 탐험을 위한 행동을 취할 확률인 앱실론의 초기값 설정.\n",
    "epsilon_init = 1.0\n",
    "\n",
    "# 앱실론의 최소값 설정.\n",
    "epsilon_min = 0.1\n",
    "\n",
    "# 소코반 환경 설정( 게임판 크기=5, 초록색 +의 수=1, 박스의 수=1)\n",
    "# 소코반 환경 : 84 X 84 RGB 이미지를 상태로 학습 수행. => test_size = [84, 84, 3]\n",
    "# 소코반 환경은 [정지, 위, 아래, 왼쪽, 오른쪽]의 5가지 행동을 하므로 action_size = 5 \n",
    "# 소코반 환경의 resetParameter 값들 설정.\n",
    "env_config = {\"gridSize\" : 5, \"numGoals\" : 1, \"numBoxs\" : 1}\n",
    "\n",
    "# 실행 날짜, 시각 설정.\n",
    "# 모델과 텐서보드 파일을 저장할 폴더를 생성할 때 폴더 이름이 중복되지 않기 위함.\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "\n",
    "# Unity 환경 경로.\n",
    "game = \"Sokoban\"\n",
    "env_name = \"../env/\" + game + \"/Windows/\" + game\n",
    "\n",
    "# 모델 저장 및 불러오기 경로,\n",
    "save_path = \"../saved_models/\" + game + \"/\" + date_time + \"_DQN\"\n",
    "load_path = \"../saved_models/\" + game + \"/20210419-13-36-21_DQN/model/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Class ==> CNN 정의 및 Loss function 설정. 네트워크 최적화 알고리즘 결정.\n",
    "class Model() :\n",
    "    def __init__(self, model_name) :\n",
    "        \n",
    "        # tf.placeholder : 나중에 값을 던져주는 공간을 만들어줌.\n",
    "        self.input = tf.placeholder(shape=[None, state_size[0], state_size[1], state_size[2]], dtype=tf.float32)\n",
    "        \n",
    "        # 학습의 안정화를 위해 -1 ~ 1 사이의 값을 가지도록 정규화.\n",
    "        self.input_nomalize = (self.input - (255 / 2)) / (255 / 2)\n",
    "        \n",
    "        # CNN 구축. ==> 3개의 CNN 충과 2개의 은닉층으로 구성.\n",
    "        with tf.compat.v1.variable_scope(name_or_scope=model_name) :\n",
    "            \n",
    "            # 합성곱 1 : 필터 크기 => 8 X 8 , 필터 수 => 32개\n",
    "            self.conv1 =  tf.layers.conv2d(inputs=self.input_nomalize, filters=32, \n",
    "                                                 activation=tf.nn.relu, kernel_size=[8,8],\n",
    "                                                 strides=[4,4], padding=\"SAME\")\n",
    "            \n",
    "            # 합성곱 2\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, \n",
    "                                                 activation=tf.nn.relu, kernel_size=[4,4],\n",
    "                                                 strides=[2,2], padding=\"SAME\")\n",
    "            \n",
    "            # 합성곱 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv1, filters=64, \n",
    "                                                 activation=tf.nn.relu, kernel_size=[3,3],\n",
    "                                                 strides=[1,1], padding=\"SAME\")\n",
    "            # 완전 연결층 1 : node = 512\n",
    "            self.flat = tf.layers.flatten(self.conv3)\n",
    "            \n",
    "            # 완전 연결층 2 : 출력층\n",
    "            self.fc1 = tf.layers.dense(self.flat, 512, activation=tf.nn.relu)\n",
    "            # 네트워크를 통해 예측한 값.\n",
    "            self.Q_Qut =  tf.layers.dense(self.fc1, action_size, activation=None)\n",
    "        self.predict = tf.argmax(self.Q_Qut, 1)\n",
    "        \n",
    "        # 학습의 목표값.\n",
    "        self.target_Q = tf.placeholder(shape=[None, action_size], dtype=tf.float32)\n",
    "        \n",
    "        # 손실함수 값 계산 및 네트워크 학습 수행.\n",
    "        # 예측한 값과 학습의 목표값 사이의 차를 줄이는 방향으로 손실함수 설정.\n",
    "        self.loss = tf.losses.huber_loos(self.target_Q, self.Q_Qut)\n",
    "        \n",
    "        # Adam 사용.\n",
    "        self.UpdateModel = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        \n",
    "        # model_name으로 시작하는 변수들만 지정.\n",
    "        self.trainable_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQNAgent class ==> DQN 알고리즘을 위한 다양한 함수 정의.\n",
    "class DQNAgent() :\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Class 함수들을 위한 값 설정.\n",
    "        # 두 개(Q, target)의 네트워크 생성.\n",
    "        self.model = Model('Q')\n",
    "        self.target_model = Model(\"target\")\n",
    "        \n",
    "        # 학습에 필요한 정보를 저장하기 위한 리플레이 메모리 생성.\n",
    "        # deque() : 자동으로 오래된 데이터를 삭제하고 새로운 데이터 추가.\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "        \n",
    "        # 연산 수행 및 그래프 실행을 위한 세션 생성. \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # 네트워크의 모든 변수 초기화.\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # 세션 수행.\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "        self.epsilon = epsilon_init\n",
    "        \n",
    "        # 학습 모델 저장 및 불러오기.\n",
    "        self.Saver = tf.train.Saver()\n",
    "        \n",
    "        # 학습한 내용을 텐서보드에 표시하기 위해 Summary 생성.\n",
    "        self.Summary, self.Merge = self.Make_Summary()\n",
    "        \n",
    "        # 학습을 시작하기 전에 타겟 네트워크를 업데이트.\n",
    "        self.update_target()\n",
    "        \n",
    "        # 로드 모드가 참이면 경로에 저장된 네트워크 모델을 불러오기.\n",
    "        if load_model == True :\n",
    "            self.Saver.restore(self.sess, load_path)\n",
    "            \n",
    "    # 에이전트 행동을 선택하는 함수.\n",
    "    def get_action(self, state) :\n",
    "        # 이와 같은 경우 조건을 충족시\n",
    "        if self.epsilon > np.random.rand():\n",
    "            \n",
    "            # 앱실론 그리드에 따라 랜덤 행동 결정.\n",
    "            return np.random.randint(0, action_size)\n",
    "        \n",
    "        else :\n",
    "            # 네트워크 연산에 따라 행동 결정.\n",
    "            predict = self.sess.run(self.model.predict, feed_dict={self.model.input:state})\n",
    "            return np.asscalar(predict)\n",
    "        \n",
    "    # [상태, 행동, 보상, 다음 상태, 게임 종료 여부] 입력 받아 사용.\n",
    "    # 리플레이 메모리 self.memory에 이 값들을 하나의 튜플로 추가.\n",
    "    def append_sample(self, state, action, reward, next_state, done) :\n",
    "        self.memory.append((state[0], action, reward, next_state[0], done))\n",
    "        \n",
    "\n",
    "    # 네트워크 모델 저장.\n",
    "    # 모델 파일은 Save_path 내부에 model이라는 폴더를 만들고 model로 저장.\n",
    "    def save_model(self) :\n",
    "        self.Saver.save(self.sess, save_path + \"/model/model\")\n",
    "        \n",
    "    \n",
    "    # 학습 수행.\n",
    "    # 미니 배치 학습을 위해 배치 데이터를 뽑고, 네트워크 학습을 수행하는 함수.\n",
    "    def train_model(self, done) :\n",
    "        # done : 게임 종료 여부가 True 라면\n",
    "        if done :\n",
    "            # 앱실론의 최솟값 보다 크다면\n",
    "            # epsilon - greedy 기법을 사용할 때 앱실론이 너무 낮은 상태로 학습하면 랜덤성이 거의 없어 탐험의 의미가 사라짐.\n",
    "            # 또한 앱실론이 큰 상태에서 학습을 끝내면 에이전트가 아직 목표를 달성하기 위한 정확한 방법을 모를 수 있음.\n",
    "            if self.epsilon > epsilon_min :\n",
    "                # (전체 학습 에피소드 - 학습 시작 에피소드)의 역수 만큼 앱실론 값 감소.\n",
    "                # 즉, 한 에피소드가 끝날 때마다 앱실론 감소.\n",
    "                self.epsilon -= 1 / (run_episode - start_train_episode)\n",
    "                \n",
    "        \n",
    "        # 학습을 위한 미니 배치 데이터 샘플링.\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # 각 데이터를 담을 리스트 생성.\n",
    "        states      = []\n",
    "        actions     = []\n",
    "        rewards     = []\n",
    "        next_states = []\n",
    "        dones       = []\n",
    "        \n",
    "        # batch_size 만큼 상태, 행동, 보상 , 다음 상태, 게임 종료 여부의 순으로 담기.\n",
    "        for i in range(batch_size) :\n",
    "            states.append(mini_batch[i][0])\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states.append(mini_batch[i][3])\n",
    "            dones.append(mini_batch[i][4])\n",
    "            \n",
    "        # 타겟값 계산.\n",
    "        target = self.sess.run(self.model.Q_Qut, feed_dict={self.model.input : states})\n",
    "        target_val = self.sess.run(self.target_model.Q_Qut, feed_dict={self.target_model.input : next_states})\n",
    "        \n",
    "        for i in range(batch_size) :\n",
    "            if dones[i] :\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "\n",
    "            else :\n",
    "                target[i][actions[i]] = rewards[i] + dicount_factor * np.amax(target_val[i])\n",
    "            \n",
    "        \n",
    "        # 학습 수행 및 손실 함수 값 계산.\n",
    "        _, loss = self.sess.run([self.model.UpdateModel, self.model.loss], feed_dict={self.model.input : states, self.model.target_Q : target})\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # 타겟 네트워크 업데이트.\n",
    "    def update_target(self) :\n",
    "        for i in range(len(self.model.trainable_var)) :\n",
    "            self.sess.run(self.target_model.trainable_var[i].assign(self.model.trainable_var[i]))\n",
    "\n",
    "\n",
    "    # 텐서 보드에 기록할 값 설정 및 데이터 기록.\n",
    "    def Make_Summary(self) :\n",
    "        self.summary_loss = tf.placeholder(dtype=tf.float32)\n",
    "        self.summary_reward = tf.placeholder(dtype=tf.float32)\n",
    "        tf.summary.scalar(\"loss\", self.summary_loss)\n",
    "        tf.summary.scalar(\"reward\", self.summary_reward)\n",
    "        Summary = tf.summary.FileWriter(logdir=save.path, graph=self.sess.graph)\n",
    "        Merge = tf.summary.merge_all()\n",
    "        \n",
    "        return Summary, Merge\n",
    "    \n",
    "    def Wirte_Summary(self, reward, loss, episode) :\n",
    "        self.Summary.add_summary(\n",
    "            self.sess.run(self.Merge, feed_dict={self.summary_loss : loss, \n",
    "                                                self.summary_reward : reward}), episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function \n",
    "- Model Class에서 정의한 네트워크와 Agent 클래스에서 정의한 다양한 함수들을 이용해 행동을 결정하고 유니티 환경과 통신하며 학습을 수행하는 함수."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step \n",
    "- 1. unity 환경 설정 & 브레인 정의.\n",
    "- 2. 행동 선택.\n",
    "- 3. Unity 환경에서 행동 수행.\n",
    "- 4. Unity 환경으로부터 다음 상태, 보상, 게임 종료 등 정보 취득.\n",
    "- 5. 학습 수행.\n",
    "- 6. 특정 스텝마다 타켓 네트워크 업데이트.\n",
    "- 7. 진행 상황 출력 및 특정 스텝마다 모델 저장.\n",
    "- 8. step2로 이동하여 2 ~ 8의 과정을 지정 횟수만큼 반복."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main 함수 ==> 전체적으로 DQN 알고리즘을 진행.\n",
    "if __name__ == '__main__' :\n",
    "    # 유니티 환경 경로 설정 (file_name)\n",
    "    env = UnityEnvironment(file_name=env_name)\n",
    "    \n",
    "    # 유니티 브레인 설정.\n",
    "    default_brain = env.brain_names[0]\n",
    "    brain = env.brains[default_brain]\n",
    "    \n",
    "    # DQNAgent 클래스를 agent로 정의.\n",
    "    agent = DQNAgent()\n",
    "    \n",
    "    step = 0 \n",
    "    rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    # 환경 설정 (env_config)에 따라 유니티 환경 리셋 및 학습 모드 설정.\n",
    "    env_info = env.reset(train_mode = train_mode, config=env_config)[default_brain]\n",
    "    \n",
    "    # 게임 진행 반복문.\n",
    "    for episode in range(run_episode + test_episode) :\n",
    "        if episode == run_episode :\n",
    "            train_mode = False\n",
    "            env_info = env.reset(train_mode = train_mode)[default_brain]\n",
    "            \n",
    "        # 상태, episode_rewards, done 초기화.\n",
    "        state = np.unit8(255 * np.array(env_info.visual_observations[0]))\n",
    "        episode_rewards = 0 \n",
    "        done = False\n",
    "        \n",
    "        # 한 에피소드를 진행하는 반복문.\n",
    "        while not done :\n",
    "            step += 1\n",
    "            \n",
    "            # 행동 결정 및 유니티 환경에 행동 적용.\n",
    "            action = agent.get_action(state, train_mode)\n",
    "            env_info = env.step(action)[default_brain]\n",
    "            \n",
    "            # 다음 상태, 보상, 게임 종료 정보 취득.\n",
    "            next_state =  np.unit8(255 * np.array(env_info.visual_observations[0]))\n",
    "            reward = env_info.rewards[0]\n",
    "            episode_rewards += reward\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            # 학습 모드인 경우 리플레이 메모리에 데이터 저장.\n",
    "            if train_mode :\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "            else :\n",
    "                time.sleep(1)\n",
    "                agent.epsilon = 0.05\n",
    "                \n",
    "            # 상태 정보 업데이트.\n",
    "            state = next_state\n",
    "            \n",
    "            if episode > start_train_episode and train_mode :\n",
    "                # 학습 수행.\n",
    "                loss = agent.train_model(done)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                # 타겟 네트워크 업데이트.\n",
    "                if step % (target_update_step) == 0 :\n",
    "                    agent.update_target()\n",
    "                    \n",
    "        rewards.append(episode_rewards)\n",
    "        \n",
    "        # 게임 진행 상황 출력 및 텐서보드에 보상과 손실함수 값 기록.\n",
    "        if episode % print_interval == 0 and episode != 0 :\n",
    "            print(f\"step : {step} episode : {episode} reward : {np.mean(rewards)} loss : {np.mean(losses)} epsilon : {agent.epsilon}\")\n",
    "            agent.Write_Summary(np.mean(rewards), np.mean(losses), episode)\n",
    "            rewards = []\n",
    "            losses = []\n",
    "            \n",
    "        # 네트워크 모델 저장.\n",
    "        if episode % save_interval == 0 and episode != 0 :\n",
    "            agent.save_model()\n",
    "            print(f\"Save Model {episode}\")\n",
    "            \n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
